---
title: "Decision Trees"
author: "Kim Pham"
date: "11/14/2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## 0. Load libraries
```{r}
library(ggplot2)
library(rpart)
library(ROCR)
library(dplyr)
library(psych)
library(caret)
```

## 1. Upload data file
Data file description: The data comes from an one level of an online geography tutoring system used by 5th grade students. The game involves:
- a pre-test of geography knowledge (pre.test),
- a series of assignments for which you have the average score (av.assignment.score), 
- the number of messages sent by each student to other students about the assignments (messages), 
- the number of forum posts students posted asking questions about the assignment (forum.posts), 
- a post test at the end of the level (post.test) 
- and whether or not the system allowed the students to go on to the next level (level.up).  

```{r}
df <- read.csv("Data/online.data.csv")
```


## 2. Visualize data

#### Comparison between pre-test and post-test scores and those did or did not level up.
```{r}
h <- ggplot(data = df, aes(x = pre.test.score, y = post.test.score)) + geom_histogram(stat="identity")
h + facet_wrap(~level.up)
```
From the visualization, we can see that those who leveled up had higher pre.test.scores than those who did not.

#### Comparison between messages, forum.posts, and av.assignment.score between all members of the course.

```{r}
h <- ggplot(data = df, aes(x = forum.posts, y = messages)) + geom_histogram(stat="identity")
h + facet_wrap(~av.assignment.score)
```
The visualization shows:
* the individuals who scored the best in av.assignment.scores are on the higher end of forum.posts usage and on the lower end of messages usage. 
* the individuals who scored the least in av.assignment.scores are on the lower end of forum.posts usage and on the lower end of messages usage. 

#### Comparison between messages, forum.posts, and av.assignment.score between members who leveled up.

```{r}
h <- ggplot(data = filter(df, level.up=="yes"), aes(x = messages, y = forum.posts)) + geom_histogram(stat="identity")
h + facet_wrap(~av.assignment.score)
```

The visualization shows those who leveled up with the highest av.assignment.score are in the mid-range of messages usage and mid-range of forum posts usage.

#### Comparison between messages, forum.posts, and av.assignment.score between members who did not level up.

```{r}
h <- ggplot(data = filter(df, level.up=="no"), aes(x = messages, y = forum.posts)) + geom_histogram(stat="identity")
h + facet_wrap(~av.assignment.score)
```

The visualization shows those who did not level up with the highest av.assignment.score are in the lower-range of messages usage and mid-range of low range of forum posts usage.

## 3. Build classification tree
```{r}
c.tree <- rpart(level.up ~ messages + forum.posts + av.assignment.score, method="class", data=df, control=rpart.control(minsplit=15, cp=.00001))
printcp(c.tree)
post(c.tree, file = "tree.ps", title = "Classification Tree")
c.tree
```

## 4. Predict probability a student levels up
```{r}
df$pred <- predict(c.tree, df, type = "prob")[,2] # type = "prob" to see the probability that our classififcation is based on.
```

## 5. Evaluate model predictions (ROC curve)
```{r}
#Plot the curve
pred.detail <- prediction(df$pred, df$level.up) 
plot(performance(pred.detail, "tpr", "fpr"))
abline(0, 1, lty = 2)

#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail,"auc"), "y.values")) #Unlist liberates the AUC value from the "performance" object created by ROCR
```

## 6. Repeat process and build new tree model with unused variables 
```{r}
c.tree1 <- rpart(level.up ~ post.test.score + pre.test.score + av.assignment.score, method="class", data=df, control=rpart.control(minsplit=15, cp=.00001))
printcp(c.tree1)
post(c.tree1, file = "tree1.ps", title = "Classification Tree")

df$pred2 <- predict(c.tree1, df, type = "prob")[,2]

pred.detail2 <- prediction(df$pred2, df$level.up) 
plot(performance(pred.detail2, "tpr", "fpr"))
abline(0, 1, lty = 2)

unlist(slot(performance(pred.detail2,"auc"), "y.values"))
```

My first model has a 'reasonable' relationship between false positives and true positive rates, whereas my second model has a 'good separation'. Since the graph the indicates a 'good separation' relationship is more desirable, the second model would be the better one to work with.

## 7. Examine Thresholds

Threshold at .8

```{r}
# Set .8 probability threshold for valid prediction 'yes', else 'no'.
df$threshold.pred1 <- ifelse(df$pred >= .8, 'yes', 'no')

# Generate confusion matrix and model diagnostics:
confusionMatrix(data = df$threshold.pred1, reference = df$level.up)
```

# Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?

Threshold at .9873418

```{r}
df$threshold.pred1 <- ifelse(df$pred >= 0.9873418, 'yes', 'no')
confusionMatrix(data = df$threshold.pred1, reference = df$level.up)
```


The Kappa calculated at the threshold .987418 has a lower Kappa than the threshold at .8. The difference is likely due to the difference in the set thresholds. So the greater the threshold, the lower the Kappa value. 

Note: A Cohen's Kappa of 1 "implies perfect agreement" (http://www.pmean.com/definitions/kappa.htm) between observed and expected agreement.